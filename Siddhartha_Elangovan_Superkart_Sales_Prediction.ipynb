{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddhartha7070/superkart-sales-prediction/blob/main/Siddhartha_Elangovan_Superkart_Sales_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klg2JF-oBblG"
      },
      "source": [
        "## **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0CcOjZ-BblL"
      },
      "source": [
        "### **Business Context**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyT6Koe7BblM"
      },
      "source": [
        "A sales forecast predicts future sales revenue based on historical data, industry trends, and the status of the current sales pipeline. Businesses use the sales forecast to estimate weekly, monthly, quarterly, and annual sales totals. A company needs to make an accurate sales forecast as it adds value across an organization and helps the different verticals to chalk out their - future course of action. Forecasting helps an organization plan its sales operations by region and provides valuable insights to the supply chain team regarding the procurement of goods and materials.\n",
        "An accurate sales forecast process has many benefits, which include improved decision-making about the future and the reduction of sales pipeline and forecast risks. Moreover, it helps to reduce the time spent in planning territory coverage and establishes benchmarks that can be used to assess trends in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm6bNQOJBblO"
      },
      "source": [
        "### **Objective**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PYtjk_YBblO"
      },
      "source": [
        "They hired you as an MLOps Engineer, and your task is to build an automated MLOps pipeline with CI/CD to deliver accurate and reliable sales forecasts. The objective is to leverage historical sales data, industry trends, and the current pipeline status to predict weekly, monthly, quarterly, and annual revenues. By automating data ingestion, preprocessing, model training, evaluation, and deployment, the pipeline will ensure scalability, consistency, and minimal manual intervention. With CI/CD integration, forecasts will be continuously updated and seamlessly deployed, enabling different business verticals to plan sales operations by region, optimize supply chain procurement, reduce risks in sales pipelines, and establish benchmarks for future trend analysis. Ultimately, this solution will enhance decision-making, streamline planning efforts, and drive operational efficiency and business growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8C11AzTBblP"
      },
      "source": [
        "### **Data Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DQx3pkaBblP"
      },
      "source": [
        "The data contains the different attributes of the various products and stores.\n",
        "*   **Product_Id**:Unique identifier of each product, each identifier having two letters at the beginning, followed by a number\n",
        "*   **Product_Weight**:Weight of each product\n",
        "*   **Product_Sugar_Content**:Sugar content of each product, like low sugar, regular, and no sugar--\n",
        "*   *Product_Allocated_Area*:Ratio of the allocated display area of each product to the total display area of all the products in a store\n",
        "*   **Product_Type**:Broad category for each product like meat, snack foods, hard drinks, dairy, canned, soft drinks, health and . hygiene, baking goods, bread, breakfast, frozen foods, fruits and vegetables, household, seafood, starchy foods, others\n",
        "*   **Product_MRP**:Maximum retail price of each product\n",
        "*   **Store_Id**:Unique identifier of each store\n",
        "*   **Store_Establishment_Year**: Year in which the store was established\n",
        "*   **Store_Size**:Size of the store, depending on sq. feet, like high, medium, and low\n",
        "*   **Store_Location_City_Type**:Type of city in which the store is located, like Tier 1, Tier 2, and Tier 3. Tier 1 consists of cities where the standard of living is comparatively higher than that of its Tier 2 and Tier 3 counterparts\n",
        "*   **Store_Type**: Type of store depending on the products that are being sold there, like Departmental Store, Supermarket Type 1, Supermarket Type 2, and Food Mart\n",
        "*   **Product_Store_Sales_Total**:Total revenue generated by the sale of that particular product in that particular store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a master folder to keep all files created when executing the below code cells\n",
        "import os\n",
        "os.makedirs(\"tourism_project\", exist_ok=True)"
      ],
      "metadata": {
        "id": "giodc4KknHID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "0LbSu_p2jYfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Registration"
      ],
      "metadata": {
        "id": "9DtS3gNDjBbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"superkart_project/data\", exist_ok=True)"
      ],
      "metadata": {
        "id": "kNUYcTe-xckI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the **data** folder created after executing the above cell, please upload the **tourism.csv** in to the folder"
      ],
      "metadata": {
        "id": "WxXiD9ZXxodF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for storing the model building files\n",
        "os.makedirs(\"superkart_project/model_building\", exist_ok=True)"
      ],
      "metadata": {
        "id": "SUKPoy0EA4jj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lLXRfi2VAgNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de54b7ed-3b74-4365-c6cb-87616ab3c2ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing superkart_project/model_building/data_register.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile superkart_project/model_building/data_register.py\n",
        "from huggingface_hub.utils import RepositoryNotFoundError, HfHubHTTPError\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "import os\n",
        "\n",
        "\n",
        "repo_id = \"siddhartha7070/superkart\"\n",
        "repo_type = \"dataset\"\n",
        "\n",
        "# Initialize API client\n",
        "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
        "\n",
        "# Step 1: Check if the space exists\n",
        "try:\n",
        "    api.repo_info(repo_id=repo_id, repo_type=repo_type)\n",
        "    print(f\"Space '{repo_id}' already exists. Using it.\")\n",
        "except RepositoryNotFoundError:\n",
        "    print(f\"Space '{repo_id}' not found. Creating new space...\")\n",
        "    create_repo(repo_id=repo_id, repo_type=repo_type, private=False)\n",
        "    print(f\"Space '{repo_id}' created.\")\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\"superkart_project/data\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=repo_type,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "hh2TjRG5WJ4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile superkart_project/model_building/prep.py\n",
        "# for data manipulation\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "# for creating a folder\n",
        "import os\n",
        "# for data preprocessing and pipeline creation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "# for hugging face space authentication to upload files\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# Define constants for the dataset and output paths\n",
        "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
        "DATASET_PATH = \"hf://datasets/siddhartha7070/superkart/superkart-sales-prediction.csv\"\n",
        "superkart_df = pd.read_csv(DATASET_PATH)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# ----------------------------\n",
        "# Define the target variable\n",
        "# ----------------------------\n",
        "# Define the target variable for the regression task\n",
        "target = 'Product_Store_Sales_Total'\n",
        "\n",
        "# List of numerical features in the dataset (excluding 'id' as it is an identifier)\n",
        "numeric_features = [\n",
        "    'Product_Weight',\n",
        "    'Product_Allocated_Area',\n",
        "    'Product_MRP',\n",
        "    'Store_Establishment_Year'\n",
        "]\n",
        "\n",
        "# List of categorical features in the dataset\n",
        "categorical_features = [\n",
        "    'Product_Sugar_Content',\n",
        "    'Product_Type',\n",
        "    'Store_Id',\n",
        "    'Store_Size',\n",
        "    'Store_Location_City_Type',\n",
        "    'Store_Type'\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# Combine features to form X (feature matrix)\n",
        "# ----------------------------\n",
        "X = superkart_df[numeric_features + categorical_features]\n",
        "\n",
        "# ----------------------------\n",
        "# Define target vector y\n",
        "# ----------------------------\n",
        "y = superkart_df[target]\n",
        "\n",
        "# ----------------------------\n",
        "# Split dataset into training and test sets\n",
        "# ----------------------------\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "Xtrain.to_csv(\"Xtrain.csv\",index=False)\n",
        "Xtest.to_csv(\"Xtest.csv\",index=False)\n",
        "ytrain.to_csv(\"ytrain.csv\",index=False)\n",
        "ytest.to_csv(\"ytest.csv\",index=False)\n",
        "\n",
        "\n",
        "files = [\"Xtrain.csv\",\"Xtest.csv\",\"ytrain.csv\",\"ytest.csv\"]\n",
        "\n",
        "for file_path in files:\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=file_path,\n",
        "        path_in_repo=file_path.split(\"/\")[-1],  # just the filename\n",
        "        repo_id=\"siddhartha7070/superkart\",\n",
        "        repo_type=\"dataset\",\n",
        "    )"
      ],
      "metadata": {
        "id": "whml0T_rBNGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669939d2-f1db-43f4-b9f1-8e637ca9337a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing superkart_project/model_building/prep.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "eZZKnLkLjeM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile superkart_project/model_building/train.py\n",
        "import pandas as pd\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import joblib\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from huggingface_hub.utils import RepositoryNotFoundError, HfHubHTTPError\n",
        "import mlflow\n",
        "import os\n",
        "\n",
        "mlflow.set_tracking_uri(\"http://localhost:8080\")\n",
        "mlflow.set_experiment(\"superkart-Package-Prediction-Experiment\")\n",
        "\n",
        "# Hugging Face API authentication\n",
        "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
        "Xtrain_path = \"hf://datasets/siddhartha7070/superkart/Xtrain.csv\"\n",
        "Xtest_path = \"hf://datasets/siddhartha7070/superkart/Xtest.csv\"\n",
        "ytrain_path = \"hf://datasets/siddhartha7070/superkart/ytrain.csv\"\n",
        "ytest_path = \"hf://datasets/siddhartha7070/superkart/ytest.csv\"\n",
        "\n",
        "# Load datasets\n",
        "Xtrain = pd.read_csv(Xtrain_path)\n",
        "Xtest = pd.read_csv(Xtest_path)\n",
        "ytrain = pd.read_csv(ytrain_path)\n",
        "ytest = pd.read_csv(ytest_path)\n",
        "\n",
        "numeric_features = [\n",
        "    'Product_Weight',\n",
        "    'Product_Allocated_Area',\n",
        "    'Product_MRP',\n",
        "    'Store_Establishment_Year'\n",
        "]\n",
        "\n",
        "# List of categorical features in the dataset\n",
        "categorical_features = [\n",
        "    'Product_Sugar_Content',\n",
        "    'Product_Type',\n",
        "    'Store_Id',\n",
        "    'Store_Size',\n",
        "    'Store_Location_City_Type',\n",
        "    'Store_Type'\n",
        "]\n",
        "\n",
        "# Set the class weight to handle class imbalance\n",
        "class_weight = ytrain.value_counts()[0] / ytrain.value_counts()[1]\n",
        "\n",
        "# Define the preprocessing steps\n",
        "preprocessor = make_column_transformer(\n",
        "    (StandardScaler(), numeric_features),\n",
        "    (OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        ")\n",
        "# Define base XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(scale_pos_weight=class_weight, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'xgbclassifier__n_estimators': [50, 75, 100, 125, 150],\n",
        "    'xgbclassifier__max_depth': [2, 3, 4],\n",
        "    'xgbclassifier__colsample_bytree': [0.4, 0.5, 0.6],\n",
        "    'xgbclassifier__colsample_bylevel': [0.4, 0.5, 0.6],\n",
        "    'xgbclassifier__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'xgbclassifier__reg_lambda': [0.4, 0.5, 0.6],\n",
        "}\n",
        "# Model pipeline\n",
        "model_pipeline = make_pipeline(preprocessor, xgb_model)\n",
        "\n",
        "# Start MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Hyperparameter tuning with GridSearchCV\n",
        "    grid_search = GridSearchCV(model_pipeline, param_grid, cv=5, n_jobs=-1)\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "\n",
        "    # Log hyperparameters\n",
        "    mlflow.log_params(grid_search.best_params_)\n",
        "\n",
        "    # Store the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Set classification threshold\n",
        "    classification_threshold = 0.45\n",
        "\n",
        "    # Make predictions on the training and test data\n",
        "    y_pred_train_proba = best_model.predict_proba(Xtrain)[:, 1]\n",
        "    y_pred_train = (y_pred_train_proba >= classification_threshold).astype(int)\n",
        "\n",
        "    y_pred_test_proba = best_model.predict_proba(Xtest)[:, 1]\n",
        "    y_pred_test = (y_pred_test_proba >= classification_threshold).astype(int)\n",
        "\n",
        "    # Evaluation\n",
        "    train_report = classification_report(ytrain, y_pred_train, output_dict=True)\n",
        "    test_report = classification_report(ytest, y_pred_test, output_dict=True)\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metrics({\n",
        "        \"train_accuracy\": train_report['accuracy'],\n",
        "        \"train_precision\": train_report['1']['precision'],\n",
        "        \"train_recall\": train_report['1']['recall'],\n",
        "        \"train_f1-score\": train_report['1']['f1-score'],\n",
        "        \"test_accuracy\": test_report['accuracy'],\n",
        "        \"test_precision\": test_report['1']['precision'],\n",
        "        \"test_recall\": test_report['1']['recall'],\n",
        "        \"test_f1-score\": test_report['1']['f1-score']\n",
        "    })\n",
        "\n",
        "    # Save the model locally\n",
        "    model_path = \"best_superkart_package_model_v1.joblib\"\n",
        "    joblib.dump(best_model, model_path)\n",
        "\n",
        "    # Log the model artifact\n",
        "    mlflow.log_artifact(model_path, artifact_path=\"model\")\n",
        "    print(f\"Model saved as artifact at: {model_path}\")\n",
        "\n",
        "    # Upload to Hugging Face\n",
        "    repo_id = \"siddhartha7070/superkart-package-model\"\n",
        "    repo_type = \"model\"\n",
        "\n",
        "    # Step 1: Check if the space exists\n",
        "    try:\n",
        "        api.repo_info(repo_id=repo_id, repo_type=repo_type)\n",
        "        print(f\"Space '{repo_id}' already exists. Using it.\")\n",
        "    except RepositoryNotFoundError:\n",
        "        print(f\"Space '{repo_id}' not found. Creating new space...\")\n",
        "        create_repo(repo_id=repo_id, repo_type=repo_type, private=False)\n",
        "        print(f\"Space '{repo_id}' created.\")\n",
        "\n",
        "    # create_repo(\"churn-model\", repo_type=\"model\", private=False)\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=\"best_superkart_model_v1.joblib\",\n",
        "        path_in_repo=\"best_superkart_model_v1.joblib\",\n",
        "        repo_id=repo_id,\n",
        "        repo_type=repo_type,\n",
        "    )"
      ],
      "metadata": {
        "id": "MrmLP6OkBbqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "377b4016-8142-4dfd-8231-1d7e0449432e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing superkart_project/model_building/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "0McYCZzkji5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dockerfile"
      ],
      "metadata": {
        "id": "9QrY2v77vbEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"superkart_project/deployment\", exist_ok=True)"
      ],
      "metadata": {
        "id": "0-AMAI72CR-T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile superkart_project/deployment/Dockerfile\n",
        "# Use a minimal base image with Python 3.9 installed\n",
        "FROM python:3.9\n",
        "\n",
        "# Set the working directory inside the container to /app\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy all files from the current directory on the host to the container's /app directory\n",
        "COPY . .\n",
        "\n",
        "# Install Python dependencies listed in requirements.txt\n",
        "RUN pip3 install -r requirements.txt\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "USER user\n",
        "ENV HOME=/home/user \\\n",
        "\tPATH=/home/user/.local/bin:$PATH\n",
        "\n",
        "WORKDIR $HOME/app\n",
        "\n",
        "COPY --chown=user . $HOME/app\n",
        "\n",
        "# Define the command to run the Streamlit app on port \"8501\" and make it accessible externally\n",
        "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]"
      ],
      "metadata": {
        "id": "ZTicTDnPCVZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476c53ab-a9a8-4f9b-d420-1ab55b54f8dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing superkart_project/deployment/Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit App"
      ],
      "metadata": {
        "id": "LCvrklrBwNvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile superkart_project/deployment/app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from huggingface_hub import hf_hub_download\n",
        "import joblib\n",
        "\n",
        "# Download the model from the Model Hub\n",
        "model_path = hf_hub_download(repo_id=\"siddhartha7070/superkart-package-model\", filename=\"best_tourism_package_model_v1.joblib\")\n",
        "\n",
        "# Load the model\n",
        "model = joblib.load(model_path)\n",
        "\n",
        "# Streamlit UI for Customer Churn Prediction\n",
        "st.title(\"Superkart Package Prediction\")\n",
        "st.write(\"Fill the customer details below to predict if they'll purchase a travel package\")\n",
        "\n",
        "# Collect user input for property features\n",
        "    Product_Weight = st.number_input(\"Product Weight\", min_value=1, value=2)\n",
        "    Product_Sugar_Content = st.number_input(\"Product_Sugar_Content\", min_value=1, value=2)\n",
        "    Product_Allocated_Area = st.number_input(\"Product_Allocated_Area\", min_value=1, value=2)\n",
        "    Product_Type = st.number_input(\"Product_Type\", min_value=1, value=2)\n",
        "    Product_MRP = st.number_input(\"Product MRP\", min_value=1, step=1, value=2)\n",
        "    Store_Id = st.selectbox(\"Store_Id\", [\"OUT001\", \"OUT002\", \"OUT003\", \"OUT004\"])\n",
        "Store_Establishment_Year = st.selectbox(\"Instantly Bookable?\", [\"False\", \"True\"])\n",
        "    Store_Size = st.selectbox(\"Store Size\", [\"Small\", \"Medium\", \"High\"])\n",
        "    Store_Location_City_Type = st.selectbox(\"Store Size\", [\"Tier 1\", \"Tier 2\", \"Tier 3\"])\n",
        "Store_Type = st.number_input(\"Store Type\", min_value=0, step=1, value=1)\n",
        "\n",
        "\n",
        "# Convert user input into a DataFrame\n",
        "input_data = pd.DataFrame([{\n",
        "    'Product_Weight': Product_Weight,\n",
        "    'Product_Sugar_Content': Product_Sugar_Content,\n",
        "    'Product_Allocated_Area': Product_Allocated_Area,\n",
        "    'Product_Type': Product_Type,\n",
        "    'Product_MRP': Product_MRP,\n",
        "    'Store_Id': Store_Id,\n",
        "    'Store_Establishment_Year': Store_Establishment_Year,\n",
        "    'Store_Size': Store_Size,\n",
        "    'Store_Location_City_Type': Store_Location_City_Type,\n",
        "    'Store_Type': Store_Type\n",
        "}])\n",
        "\n",
        "# Set the classification threshold\n",
        "classification_threshold = 0.45\n",
        "\n",
        "# Predict button\n",
        "if st.button(\"Predict\"):\n",
        "    prob = model.predict_proba(input_data)[0,1]\n",
        "    pred = int(prob >= classification_threshold)\n",
        "    result = \"Predicted Sales Price (in dollars\" if pred == 1 else \"Error making prediction\"\n",
        "    st.write(f\"Prediction:  Sales Price (in dollars) {result}\")"
      ],
      "metadata": {
        "id": "jzge_a2LCdWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b66570e-9eff-4efa-9a6f-0bf3f8f15f02"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing superkart_project/deployment/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Handling"
      ],
      "metadata": {
        "id": "07cYzWcIwTL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile superkart_project/deployment/requirements.txt\n",
        "pandas==2.2.2\n",
        "huggingface_hub==0.32.6\n",
        "streamlit==1.43.2\n",
        "joblib==1.5.1\n",
        "scikit-learn==1.6.0\n",
        "xgboost==2.1.4\n",
        "mlflow==3.0.1"
      ],
      "metadata": {
        "id": "NgW5sT83ChI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9092c2fd-5957-4d4e-ed5b-3c192b8faa79"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing superkart_project/deployment/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hosting"
      ],
      "metadata": {
        "id": "V4ynzpKNwWS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"superkart_project/hosting\", exist_ok=True)"
      ],
      "metadata": {
        "id": "Blkwc3ggByKm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile superkart_project/hosting/hosting.py\n",
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
        "api.upload_folder(\n",
        "    folder_path=\"tourism_project/deployment\",     # the local folder containing your files\n",
        "    repo_id=\"siddhartha7070/superkart-Package-Prediction\",          # the target repo\n",
        "    repo_type=\"space\",                      # dataset, model, or space\n",
        "    path_in_repo=\"\",                          # optional: subfolder path inside the repo\n",
        ")"
      ],
      "metadata": {
        "id": "fV6vwNqiBwrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac1a923-e0f6-41e0-98b5-269dc277da70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing superkart_project/hosting/hosting.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create MLOps pipeline with Github Action Workflow"
      ],
      "metadata": {
        "id": "PuCgAW2hktli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "name: Superkart Project Pipeline\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches:\n",
        "      - main  # Automatically triggers on push to the main branch\n",
        "\n",
        "jobs:\n",
        "\n",
        "  register-dataset:\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "      - uses: actions/checkout@v3\n",
        "      - name: Install Dependencies\n",
        "        run: pip install -r superkart_project/requirements.txt\n",
        "      - name: Upload Dataset to Hugging Face Hub\n",
        "        env:\n",
        "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
        "        run: python superkart_project/model_building/data_register.py\n",
        "\n",
        "  data-prep:\n",
        "    needs: register-dataset\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "      - uses: actions/checkout@v3\n",
        "      - name: Install Dependencies\n",
        "        run: pip install -r superkart_project/requirements.txt\n",
        "      - name: Run Data Preparation\n",
        "        env:\n",
        "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
        "        run: python superkart_project/model_building/prep.py\n",
        "\n",
        "\n",
        "  model-traning:\n",
        "    needs: data-prep\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "      - uses: actions/checkout@v3\n",
        "      - name: Install Dependencies\n",
        "        run: pip install -r superkart_project/requirements.txt\n",
        "      - name: Start MLflow Server\n",
        "        run: |\n",
        "          nohup mlflow ui --host 0.0.0.0 --port 5000 &  # Run MLflow UI in the background\n",
        "          sleep 5  # Wait for a moment to let the server starts\n",
        "      - name: Model Building\n",
        "        env:\n",
        "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
        "        run: python superkart_project/model_building/train.py\n",
        "\n",
        "\n",
        "  deploy-hosting:\n",
        "    runs-on: ubuntu-latest\n",
        "    needs: [model-traning,data-prep,register-dataset]\n",
        "    steps:\n",
        "      - uses: actions/checkout@v3\n",
        "      - name: Install Dependencies\n",
        "        run: pip install -r superkart_project/requirements.txt\n",
        "      - name: Push files to Frontend Hugging Face Space\n",
        "        env:\n",
        "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
        "        run: python superkart_project/hosting/hosting.py\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "J029tYPq4Rmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** To use this YAML file for our use case, we need to\n",
        "\n",
        "1. Go to the GitHub repository for the project\n",
        "2. Create a folder named ***.github/workflows/***\n",
        "3. In the above folder, create a file named ***pipeline.yml***\n",
        "4. Copy and paste the above content for the YAML file into the ***pipeline.yml*** file"
      ],
      "metadata": {
        "id": "T9fgZ_Mq3zzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements file for the Github Action Workflow"
      ],
      "metadata": {
        "id": "PvEUJ-t5kdxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile superkart_project/requirements.txt\n",
        "huggingface_hub==0.32.6\n",
        "datasets==3.6.0\n",
        "pandas==2.2.2\n",
        "scikit-learn==1.6.0\n",
        "xgboost==2.1.4\n",
        "mlflow"
      ],
      "metadata": {
        "id": "tePQBrGOCn2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5e5581-2e20-4882-910d-dc94fef7096a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing superkart_project/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Github Authentication and Push Files"
      ],
      "metadata": {
        "id": "BA6mP-Ebkm3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Before moving forward, we need to generate a secret token to push files directly from Colab to the GitHub repository.\n",
        "* Please follow the below instructions to create the GitHub token:\n",
        "    - Open your GitHub profile.\n",
        "    - Click on ***Settings***.\n",
        "    - Go to ***Developer Settings***.\n",
        "    - Expand the ***Personal access tokens*** section and select ***Tokens (classic)***.\n",
        "    - Click ***Generate new token***, then choose ***Generate new token (classic)***.\n",
        "    - Add a note and select all required scopes.\n",
        "    - Click ***Generate token***.\n",
        "    - Copy the generated token and store it safely in a notepad."
      ],
      "metadata": {
        "id": "T84Ei-g9Z2uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Git\n",
        "!apt-get install git\n",
        "\n",
        "# Set your Git identity (replace with your details)\n",
        "!git config --global user.email \"siddhartha.7070@gmail.com\"\n",
        "!git config --global user.name \"siddhartha7070\"\n",
        "\n",
        "# Clone your GitHub repository\n",
        "!git clone https://github.com/siddhartha7070/superkart_sales_prediction.git\n",
        "\n",
        "# Move your folder to the repository directory\n",
        "!mv /content/superkart_project/ /content/superkart_sales_prediction"
      ],
      "metadata": {
        "id": "KPDx4gqGh7cO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc8b8d9-7b15-4a47-fb71-cc2606916f4c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Cloning into 'superkart_sales_prediction'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to the cloned repository\n",
        "%cd superkart_sales_prediction/\n",
        "\n",
        "# Add the new folder to Git\n",
        "!git add .\n",
        "\n",
        "# Commit the changes\n",
        "!git commit -m \"first commit\"\n",
        "\n",
        "# Push to GitHub (you'll need your GitHub credentials; use a personal access token if 2FA enabled)\n",
        "!git push https://siddhartha7070:siddhartha.7070@github.com/siddhartha7070/superkart_sales_prediction.git"
      ],
      "metadata": {
        "id": "IuUahCwVigon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e44320a-fa87-4edc-f1a6-b8f925031f8b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/superkart_sales_prediction\n",
            "[main f78268d] first commit\n",
            " 8 files changed, 332 insertions(+)\n",
            " create mode 100644 superkart_project/deployment/Dockerfile\n",
            " create mode 100644 superkart_project/deployment/app.py\n",
            " create mode 100644 superkart_project/deployment/requirements.txt\n",
            " create mode 100644 superkart_project/hosting/hosting.py\n",
            " create mode 100644 superkart_project/model_building/data_register.py\n",
            " create mode 100644 superkart_project/model_building/prep.py\n",
            " create mode 100644 superkart_project/model_building/train.py\n",
            " create mode 100644 superkart_project/requirements.txt\n",
            "remote: Invalid username or token. Password authentication is not supported for Git operations.\n",
            "fatal: Authentication failed for 'https://github.com/siddhartha7070/superkart_sales_prediction.git/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=6 color=\"navyblue\">Power Ahead!</font>\n",
        "___"
      ],
      "metadata": {
        "id": "fN8j9-3nW8G9"
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "klg2JF-oBblG",
        "m0CcOjZ-BblL",
        "zm6bNQOJBblO",
        "z8C11AzTBblP",
        "0LbSu_p2jYfe",
        "9DtS3gNDjBbR",
        "hh2TjRG5WJ4Z",
        "eZZKnLkLjeM4",
        "0McYCZzkji5I",
        "9QrY2v77vbEZ",
        "LCvrklrBwNvJ",
        "07cYzWcIwTL-",
        "V4ynzpKNwWS_",
        "PuCgAW2hktli",
        "PvEUJ-t5kdxH",
        "BA6mP-Ebkm3O"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}